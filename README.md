# ğŸš€ GPT-Reimagined: KANs vs MLPs

> Exploring Kolmogorov-Arnold Networks as a drop-in replacement for MLPs inside GPT architectures.

This repository implements a **Generative Pre-trained Transformer (GPT)** model using two alternative feed-forward architectures:

- Traditional **Multilayer Perceptrons (MLPs)**
- Novel **Kolmogorov-Arnold Networks (KANs)**

We experimentally evaluate whether **KAN layers can outperform classical MLP blocks** in convergence, expressiveness, and efficiency for language modeling.

---

## ğŸ§  Why KANs?

The **Kolmogorov-Arnold Representation Theorem** states that any multivariate continuous function can be decomposed into a sum of univariate functions.

KANs exploit this principle to build neural networks that are:

- More interpretable  
- Parameter efficient  
- Expressive with fewer hidden units  

This project investigates whether **KANs can replace MLPs inside GPT architectures without sacrificing language generation quality.**

---

## ğŸ—ï¸ KAN-GPT Architecture

<div align="center">
  <img src="./images/kan-gpt.png" width="700"/>
</div>

---

## âš™ï¸ Tech Stack

| Category | Tools |
|--------|------|
| Language | Python |
| Framework | PyTorch |
| Libraries | NumPy, Pandas, SciPy, tqdm, tiktoken |
| Visualization | Matplotlib, TensorBoard |
| Datasets | Tiny Shakespeare, WikiText-2 |
| Tools | Git, Google Colab, Kaggle |

---

## ğŸ¯ Objectives

- Implement GPT using traditional MLP blocks  
- Implement GPT using Kolmogorov-Arnold Networks  
- Compare both approaches on:
  - Training convergence  
  - Validation loss & perplexity  
  - Text generation quality  
  - Parameter efficiency  
  - Resource utilization  

---

# File Directory

<pre><code>
GPT-Reimagined/
â”œâ”€â”€ data/                          # Dataset (tiny Shakespeare data used here)
â”‚   â”œâ”€â”€ tinyshakespeare/
â”‚   â”‚   â”œâ”€â”€ input.txt              # Encoded input data
â”‚   â”‚   â”œâ”€â”€ train.bin              # Encoded training data
â”‚   â”‚   â”œâ”€â”€ val.bin                # Encoded validation data
â”œâ”€â”€ models/                        # Directory for saving trained models
â”œâ”€â”€ logs/                          # Training logs for TensorBoard
â”œâ”€â”€ archive_logs/                  # Archive of zipped logs
â”œâ”€â”€ main.py                        # Main script to initiate training
â”œâ”€â”€ dataset_shakespeare.py         # Data processing and loading script
â”œâ”€â”€ model_kan.py                   # Kolmogorov-Arnold Network (KAN) model
â”œâ”€â”€ model_mlp.py                   # MLP-based GPT model
â”œâ”€â”€ train.py                       # Training loop for the models
â”œâ”€â”€ config.py                      # Configuration for hyperparameters and paths
â”œâ”€â”€ generate.py                    # Script for generating text with the trained model
â”œâ”€â”€ utils.py                       # Utility functions
â”œâ”€â”€ requirements.txt               # Required dependencies
â””â”€â”€ README.md                      # This README file
</code></pre>

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/GPT-Reimagined.git
cd GPT-Reimagined
pip install -r requirements.txt
Dataset is automatically downloaded and tokenized using dataset_shakespeare.py.

â–¶ï¸ Training
bash
Copy code
python main.py
View logs using:

bash
Copy code
tensorboard --logdir=logs
âœï¸ Text Generation
bash
Copy code
python generate.py
Control output length using max_new_tokens in generate.py.

ğŸ”¬ Experiment Configuration
Parameter	Value
Block Size	64
Batch Size	64
Learning Rate	2e-5
Epochs	6
Loss	Cross-Entropy
Metric	Validation Loss, Perplexity

ğŸ“Š Results
KAN-GPT Generated Sample


KAN-based GPT shows:

Faster early convergence

Better coherence at low epochs

Reduced overfitting on small datasets

ğŸ§ª Mini Projects
MNIST Classification (MLP vs KAN)

FashionMNIST CNN Classifier

NameGPT

Masked Language Model

Transformer-based Translator

ğŸ‘¥ Contributors
Vineet Unde

Shraddha Bhadane
